---
title: "Own Project Report: Predicting Organic Carbon in Soils"
author: "Marcus Schmidt"
date: "23/02/2021"
output: pdf_document
---

# (I) INTRODUCTION

## (I a) Background, goal & data set

Soils are the basis of all agriculture. Organic carbon and its dynamics plays a large role carbon sequestration and therefore helps regulate our climate. The goal of this project is to predict soil organic carbon from a large set of European soil data. The data set is from the LUCAS initiative 2015 (https://esdac.jrc.ec.europa.eu/projects/lucas) and includes over 20,000 sampling points on land that is used in different ways.

## (I b) Data set download

The data set was requested online from the LUCAS initiative and the author got permission to use it for this project. Out of the data set, I created a *.Rdata file to be downloaded here:
```{r, echo = T, results = "hide"}
load(url("https://github.com/ms-soil/ds-submission-soil/raw/main/eusoil.Rdata"))
```

## (I c) Data set structure

Let's take a peak at the datset, showing the variables included, the dimension of the data set and a first look at the numbers and their variable type:

```{r include=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")

library(caret)
library(tidyverse)
library(randomForest)
```

```{r, echo = T}
names(eusoil) # variable names
dim(eusoil) # data set dimensions
```

\newpage

```{r, echo = T}
as_tibble(eusoil) # overview of data structure
```

# (II) METHODS & ANALYSIS

## (II a) Data preparation

Analysis was don with R version 4.0.3.
Land use in the data set is not straightforward. It is decoded in the LC1-variable, so we detect the sting and use its explanation from the data-set documentation, which can be found at https://github.com/ms-soil/ds-submission-soil (data-info.pdf). After this we remove the LC1-variable. We further check whether there are any observations without organic carbon (OC) and take a look at how many observations we have for each land use. In this step, we finally factorize land use.

```{r, echo = T}
eusoil <- eusoil %>% mutate(landuse = case_when(str_detect(LC1, "A") ~ "artif_land",
                                                str_detect(LC1, "B") ~ "cropland",
                                                str_detect(LC1, "C") ~ "woodland",
                                                str_detect(LC1, "D") ~ "shrubland",
                                                str_detect(LC1, "E") ~ "grassland",
                                                str_detect(LC1, "F") ~ "bare_land",
                                                str_detect(LC1, "G") ~ "water",
                                                str_detect(LC1, "H") ~ "wetland")) %>% 
  select(-LC1)

table(is.na(eusoil$OC)) # organic carbon data is available in all observations

# distribution of landuses
table(eusoil$landuse)

# factorize landuse
eusoil <- eusoil %>% mutate(landuse = as.factor(landuse))
```



## (II b) Variable selection

We want to use a complete data set for our predictions, so we exclude variables where we don't have all observation. We can see it with this code. It yields that we are missing the texture variables in over 80% of cases.
```{r, echo = T, results = "hide"}
# print availability for every variable
res <- for (i in 1:length(names(eusoil))) { 
  print(table(!is.na(eusoil[,i])))
  print(names(eusoil[i]))
}
# texture (the silt, sand and clay variables) are only available in a fraction of cases
# see which fraction of the dataset misses texture
condition <- !is.na(eusoil$Clay)
table(condition)[[1]]/(table(condition)[[1]] + table(condition)[[2]])
# texture is missing in 80.5% of cases
```
Here is a bit of the result:
```{r, echo = T}
# FALSE  TRUE 
# 17599  4260 
# [1] "Clay"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Sand"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Silt"
```


The texture variables clay, sand and silt are not often measured in this data set so we exclude them, as well as some other variables that are not commonly measured by soil scientists or not clearly defined:

```{r, echo = T}
eusoil <- eusoil %>% select(-Clay, -Sand, -Silt, -EC, -pH.CaCl2., -Soil_Stones, -LC1_Desc) %>% 
  select(OC, N, P, K, CaCO3, pH.H2O., Elevation, landuse)
```

What variables are left and how do they relate. We want to draw some conclusions of what to include by looking at a correlation plot.
```{r, echo = T, message = F, results = "hide"}
#### variable selection III: influential variables ####
names(eusoil)
## a) numerical variables that correlate with organic carbon but not with each other
relationships <- plot(eusoil[1:2000,]) # plotting the first 2000 observations
relationships
```
P (phosphorus), K (potassium) show no clear relationship to organic carbon so we will exclude them soon. CaCO3 and pH are co-dependend so it will be enough to include one of them. What remains at this point is N (Nitrogen), pH and elevation.

There is also a categorical variable, land use and want to see whether it influences organic carbon:
```{r, echo = T}
qplot(landuse, OC, data = eusoil, fill = landuse) + geom_boxplot() + 
  ylab("organic carbon mg C / g soil")
```
Landuse plays a role so we keep it. Finally, we give out some easier-to-type variable names and select what we found important
```{r, echo = T}
eusoil <- eusoil %>% mutate(pH = pH.H2O., elev = Elevation) %>% 
  select(OC, N, P, pH, elev, landuse)
```

## (II c) Data set division into validation, train and test set

First, from the created data, a validation set of 10% is taken so we have a large proportion of the data for model training. The training set will be 80% and the test set 20% of the remaining data. We should choose a split that allows lots of training but still a representative test set. For reproducible results in drawing, we use the set.seed() function.

```{r, echo = T, warning = F}
set.seed(1, sample.kind = "Rounding")
valindex <- createDataPartition(eusoil$OC, p = 0.1, list = F)
temp <- eusoil[-valindex,]
validation <- eusoil[valindex,]

set.seed(1, sample.kind = "Rounding")
testindex <- createDataPartition(temp$OC, p = 0.2, list = F)
trainset <- temp[-testindex,]
testset <- temp[testindex,]

# see how many observations are in each
nrow(trainset); nrow(testset); nrow(validation)
```
Over 2000 obervations for final validation set should be plenty.

## (II d) RMSE function

A rooted mean square error (RMSE) function is used to evaluate the performance of the models that will be set up.
```{r, echo = T}
RMSE <- function(observed_values, predicted_values){
  sqrt(mean((observed_values - predicted_values)^2))
}
```


## (II e) Reference model
A reference model can be useful to evaluate, how different the models perform from simply guessing, or taking the average of the observed carbon values.

```{r echo = T, fig.height=3, fig.width=3}
#### guessing model ####
mu <- mean(trainset$OC)

rmse_mu <- RMSE(testset$OC, mu)
rmse_mu

# create an RMSE table to always add the RMSEs
rmses <- data.frame(model = "mean", rmse = rmse_mu)
rmses
```
So if we guess, we are typically ~78 mg C / g soil off. We will improve this prediction, first with linear models but also with the KNN and random forest algorithms.




## (II f) Linear models
In the following, I try different linear models, first with N (nitrogen), then adding pH, elevation and landuse. The full code is present in the R script, to not overwhelm the report, I show the model with all these variables and the table of how RMSE impoved step by step.

```{r, include = F}
#### LM with N (Nitrogen) ####

m1 <- lm(OC ~ N, data = trainset) 

# the base will be the testset as a dataframe to predict on
base <- data.frame(testset)
pred1 <- predict(m1, base)
head(pred1)

# view predictions
qplot(testset$OC, pred1, col = testset$landuse)

# get RMSE
rmse1 <- RMSE(testset$OC, pred1)
rmse1

# add RMSE to RMSE table
rmses <- rbind(rmses, 
               data.frame(model = "lm with N", rmse = rmse1))
rmses

#### LM with N, pH #### 
# for comments on the steps, see above

m2 <- lm(OC ~ N + pH, data = trainset)
summary(m2)

base <- data.frame(testset)

pred2 <- predict(m2, base)
head(pred2)

qplot(testset$OC, pred2)
rmse2 <- RMSE(testset$OC, pred2)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH", rmse = rmse2))
rmses


#### LM with N, pH, elevation ####

m3 <- lm(OC~N + pH + elev, data = trainset)
summary(m3)

base <- data.frame(testset)

pred3 <- predict(m3, base)
head(pred3)

qplot(testset$OC, pred3)
rmse3 <- RMSE(testset$OC, pred3)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH + elev.", rmse = rmse3))
rmses
```

```{r echo = T, fig.height=3, fig.width=3}
#### lm with N, pH, elevation, landuse ####

m_all <- lm(OC ~ ., data = trainset)

base <- data.frame(testset)

pred4 <- predict(m_all, base)
head(pred4)

qplot(testset$OC, pred4)
rmse4 <- RMSE(testset$OC, pred4)
rmse4

rmses <- rbind(rmses, 
               data.frame(model = "lm with all variables", rmse = rmse4))
rmses

```

\newpage

## (II g) KNN models
Since all variables appeared to improve the model, I also include these in a KNN model, which looks multidimensionally at the nearest neighbors of an observation and predicts from there.
A first try including elevation yields a less-than-optimal result (see RMSE table below), but excluding elevation worked well. I could not immediately detect the reason for this, but my conclusion is that not every variable improves every type of model in the same way. Generally, knn performed well after improving on the number of neighbors with a sapply() function.

```{r, include = F}
#### KNN [caret] with N, pH, elevation, landuse ####
# as a rule of thumb, sqrt of observations:
sqrt(nrow(trainset)) # = 125

# after some trials, the following range of ks were found useful
ks <- seq(1,15,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn <- knnreg(OC ~ ., data = trainset, k = i)
  pred <-predict(fit_knn, base)
  RMSE(testset$OC, pred)
})
data.frame(result)[1,]

plot(ks, result)

best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 3
# it appears a better k is lower but I don't want to depend on too few points as this
# may lead to overtraining

best_k <- 10

## using best k

fit_knn <- knnreg(OC ~ ., data = trainset, k = best_k)
pred6 <-predict(fit_knn, base)

qplot(testset$OC, pred6)
rmse6 <- RMSE(testset$OC, pred6)
rmse6

rmses <- rbind(rmses, 
               data.frame(model = "knn with all variables", rmse = rmse6))
rmses
# this is clearly not good
# elevation showed to be the problem
```

```{r echo = T, fig.height=3, fig.width=3}

#### KNN [caret] all variables / excl. elevation ####
sqrt(nrow(trainset)) 

ks <- seq(10,30,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = i)
  pred <-predict(fit_knn2, base)
  RMSE(testset$OC, pred)
})

plot(ks, result)

best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 17

## using best k

fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = best_k)
pred7 <-predict(fit_knn2, base)

qplot(testset$OC, pred7)
rmse7 <- RMSE(testset$OC, pred7)
rmse7

rmses <- rbind(rmses, 
               data.frame(model = "knn excluding elev.", rmse = rmse7))
rmses

```

\newpage

## (II h) Random forest model

A random forest model consists of many models (trees) that are combined (to a forest) so the number of trees plays a role. The more trees the more exact the model usually is, but it then also takes more time to calculate. Here, I started with 10 trees, then tried 100, then 250 and then 500. The number of trees (ntree = x) of 250 appeard to yield a good compromise between computing time and performance. The best model also includes all variables, unlike KNN did.

```{r echo = T, fig.height=3, fig.width=3}
set.seed(1)

fit_rf <- randomForest(
  OC ~ ., data = trainset, 
  ntree = 250
)

# going from 10 to 100 trees in the only-N model improved the RMSE by ~0.4
# and another 0.2 for 250 trees, hardly then anymore for 500 trees

pred8 <- predict(fit_rf, base)
head(pred8)

qplot(testset$OC, pred8)
rmse8 <- RMSE(testset$OC, pred8)
rmse8

rmses <- rbind(rmses, 
               data.frame(model = "rf with all variables", rmse = rmse8))
rmses

## in this case all variables improve the model


```

## (II i) Ensemble model (KNN and random forest)
The best two models were combined into an ensemble, where the mean is taken from the prediction of both. This works well as it improves the RMSE again:

```{r echo=T, fig.height=3, fig.width=3}
pred_ensemble <- (pred7 + pred8)/2 # knn & random forest 

qplot(testset$OC, pred_ensemble)
rmse_ens1 <- RMSE(testset$OC, pred_ensemble)
rmse_ens1

rmses <- rbind(rmses, 
               data.frame(model = "ensemble of knn & rf", rmse = rmse_ens1))
```

\newpage

# (III) RESULTS

## (III a) RMSEs of tested models

As a result, we have an order of performance from best to least performing model that is Ensemble > KNN > Random forest > linear model.

Here's the overview table:

```{r, echo = T}
rmses
```

## (III b) Evaluation of best model on validation set

I now predict organic Carbon in the validation set with the best two models: KNN without elevation and Random forest with all variables.

```{r echo = T, fig.height=3, fig.width=5}
# incorporate the mean prediction of knn and rf since they are the best ones

fin_predict_a <- predict(fit_knn2, validation)
fin_predict_b <- predict(fit_rf, validation)

fin_predict <- (fin_predict_a + fin_predict_b) / 2

qplot(validation$OC, fin_predict, col = validation$landuse)

fin_rmse <- RMSE(validation$OC, fin_predict)

rmses <- rbind(rmses, 
               data.frame(model = "FINAL RMSE OVERALL", rmse = fin_rmse))
```

#### This yields the final RMSE, our typical prediction error, which is:

```{r, echo = T}
fin_rmse
```

# (III b) Model performance in the most common range of organic carbon




# (IV) CONCLUSION 


## (IV a) Lessons learned

## (IV b) Limitations

## (IV c) Final thoughts


\newpage