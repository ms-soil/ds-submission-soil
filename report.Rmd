---
title: "Own Project Report: Predicting Organic Carbon in Soils"
author: "Marcus Schmidt"
date: "23/02/2021"
output: pdf_document
---

# (I) INTRODUCTION

## (I a) Background, goal & data set

Soils are the basis of all agriculture. Organic carbon and its dynamics plays a large role carbon sequestration and therefore helps regulate our climate. The goal of this project is to predict soil organic carbon from a large set of European soil data. The data set is from the LUCAS initiative 2015 (https://esdac.jrc.ec.europa.eu/projects/lucas) and includes over 20,000 sampling points on land that is used in different ways.

## (I b) Data set download

The data set was requested online from the LUCAS initiative and the author got permission to use it for this project. Out of the data set, I created a *.Rdata file to be downloaded here:
```{r, echo = T, results = "hide"}
load(url("https://github.com/ms-soil/ds-submission-soil/raw/main/eusoil.Rdata"))
```

## (I c) Data set structure

Let's take a peak at the datset, showing the variables included, the dimension of the data set and a first look at the numbers and their variable type:

```{r include=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")

library(caret)
library(tidyverse)
library(randomForest)
```

```{r, echo = T}
names(eusoil) # variable names
dim(eusoil) # data set dimensions
```

\newpage

```{r, echo = T}
as_tibble(eusoil) # overview of data structure
```

# (II) METHODS & ANALYSIS

## (II a) Data preparation

Analysis was don with R version 4.0.3.
Land use as a variable in the data set is not straightforward. It is decoded in the LC1-variable, so we detect the capital letter in the variable character string and assign the respective land use from the data-set documentation, which can be found at https://github.com/ms-soil/ds-submission-soil (data-info.pdf). After this we remove the LC1-variable. We further check whether there are any observations where organic carbon (OC) is missing and take a look at how many observations we have for each land use. For further analysis, land use will be turned into a factor.

```{r, echo = T}
eusoil <- eusoil %>% mutate(landuse = case_when(str_detect(LC1, "A") ~ "artif_land",
                                                str_detect(LC1, "B") ~ "cropland",
                                                str_detect(LC1, "C") ~ "woodland",
                                                str_detect(LC1, "D") ~ "shrubland",
                                                str_detect(LC1, "E") ~ "grassland",
                                                str_detect(LC1, "F") ~ "bare_land",
                                                str_detect(LC1, "G") ~ "water",
                                                str_detect(LC1, "H") ~ "wetland")) %>% 
  select(-LC1)

table(is.na(eusoil$OC)) # organic carbon data is available in all observations

# distribution of landuses
table(eusoil$landuse)

# factorize landuse
eusoil <- eusoil %>% mutate(landuse = as.factor(landuse))
```



## (II b) Variable selection

We want to use a complete data set for our predictions, so we exclude variables that are often missing. We can see it with the following code. It yields that we are missing the texture variables (sand, silt, clay) in over 80% of cases.
```{r, echo = T, results = "hide"}
# print availability for every variable
res <- for (i in 1:length(names(eusoil))) { 
  print(table(!is.na(eusoil[,i])))
  print(names(eusoil[i]))
}
# texture (the silt, sand and clay variables) are only available in a fraction of cases
# see which fraction of the dataset misses texture
condition <- !is.na(eusoil$Clay)
table(condition)[[1]]/(table(condition)[[1]] + table(condition)[[2]])
# texture is missing in 80.5% of cases
```
Here is a bit of the result:
```{r, echo = T}
# FALSE  TRUE 
# 17599  4260 
# [1] "Clay"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Sand"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Silt"
```

The texture variables clay, sand and silt are not often measured in this data set so we exclude them. We also exclude other variables that are not commonly measured by soil scientists or not clearly defined:

```{r, echo = T}
eusoil <- eusoil %>% select(-Clay, -Sand, -Silt, -EC, -pH.CaCl2., -Soil_Stones, -LC1_Desc) %>% 
  select(OC, N, P, K, CaCO3, pH.H2O., Elevation, landuse)
```

What variables are left and how do they relate? We want to draw some conclusions of what to include by looking at a correlation plot.

```{r, echo = T, message = F, results = "hide"}
#### variable selection III: influential variables ####
names(eusoil)
## a) numerical variables that correlate with organic carbon but not with each other
relationships <- plot(eusoil[1:2000,]) # plotting the first 2000 observations
relationships
```

P (phosphorus) and K (potassium) show no clear relationship to organic carbon so we will exclude them soon in the code below. CaCO3 and pH are co-dependend so it will be enough to include one of them. Thus we are keeping N (Nitrogen), pH and elevation in our analysis

There is also a categorical variable, land use, so we check whether it influences organic carbon:

```{r, echo = T}
qplot(landuse, OC, data = eusoil, fill = landuse) + geom_boxplot() + 
  ylab("organic carbon mg C / g soil")
```

Organic carbon differs with land use so we keep it as a factor. Now that the decision on the variables to consider has been made, we give out some easier-to-type variable names and select the chosen variables from the data set.

```{r, echo = T}
eusoil <- eusoil %>% mutate(pH = pH.H2O., elev = Elevation) %>% 
  select(OC, N, P, pH, elev, landuse)
```

## (II c) Data set division into validation, train and test set

First, from our data set, a validation set of 10% is taken which will only be used after deciding on a model. The validation set is important so that our model will not be fit to a specific data set. We keep a large proportion of the data for model training. The training set will be 80% and the test set 20% of the remaining data. We should choose a split that allows lots of training but still a representative test set. For reproducible results, we use the set.seed() function.

```{r, echo = T, warning = F}
set.seed(1, sample.kind = "Rounding")
valindex <- createDataPartition(eusoil$OC, p = 0.1, list = F)
temp <- eusoil[-valindex,]
validation <- eusoil[valindex,]

set.seed(1, sample.kind = "Rounding")
testindex <- createDataPartition(temp$OC, p = 0.2, list = F)
trainset <- temp[-testindex,]
testset <- temp[testindex,]

# see how many observations are in each
nrow(trainset); nrow(testset); nrow(validation)
```
The split retains 2000 obervations for final validation set which should be plenty.

## (II d) RMSE function

A rooted mean square error (RMSE) function is used to evaluate the performance of the models that will be set up. It can be viewed as the typical error we make when predicting from a given model.

```{r, echo = T}
RMSE <- function(observed_values, predicted_values){
  sqrt(mean((observed_values - predicted_values)^2))
}
```


## (II e) Reference model
A reference model can be useful to evaluate how different models perform compared to simply guessing the outcome. In guessing, we would take the average of the observed carbon values.

```{r echo = T, fig.height=3, fig.width=3}
#### guessing model ####
mu <- mean(trainset$OC)

rmse_mu <- RMSE(testset$OC, mu)
rmse_mu

# create an RMSE table to always add the RMSEs
rmses <- data.frame(model = "mean", rmse = rmse_mu)
rmses
```

We see that if we guess, we are typically ~78 mg C / g soil off the observed value. We will improve this prediction, first with linear models but also with KNN and random forest algorithms.


## (II f) Linear models

In the following, different linear models (LM) are set up, first with N (nitrogen) as predictor, then adding pH, elevation and landuse. The full code is present in the R script and not shown here as to not overload the report, However, we show the LM which includes all variables and the table of how RMSE impoved step by step when adding more variables.

```{r, include = F}
#### LM with N (Nitrogen) ####

m1 <- lm(OC ~ N, data = trainset) 

# the base will be the testset as a dataframe to predict on
base <- data.frame(testset)
pred1 <- predict(m1, base)
head(pred1)

# view predictions
qplot(testset$OC, pred1, col = testset$landuse)

# get RMSE
rmse1 <- RMSE(testset$OC, pred1)
rmse1

# add RMSE to RMSE table
rmses <- rbind(rmses, 
               data.frame(model = "lm with N", rmse = rmse1))
rmses

#### LM with N, pH #### 
# for comments on the steps, see above

m2 <- lm(OC ~ N + pH, data = trainset)
summary(m2)

base <- data.frame(testset)

pred2 <- predict(m2, base)
head(pred2)

qplot(testset$OC, pred2)
rmse2 <- RMSE(testset$OC, pred2)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH", rmse = rmse2))
rmses


#### LM with N, pH, elevation ####

m3 <- lm(OC~N + pH + elev, data = trainset)
summary(m3)

base <- data.frame(testset)

pred3 <- predict(m3, base)
head(pred3)

qplot(testset$OC, pred3)
rmse3 <- RMSE(testset$OC, pred3)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH + elev.", rmse = rmse3))
rmses
```

```{r echo = T, fig.height=3, fig.width=3}
#### lm with N, pH, elevation, landuse ####

m_all <- lm(OC ~ ., data = trainset)

base <- data.frame(testset)

pred4 <- predict(m_all, base)
head(pred4)

qplot(testset$OC, pred4)
rmse4 <- RMSE(testset$OC, pred4)
rmse4

rmses <- rbind(rmses, 
               data.frame(model = "lm with all variables", rmse = rmse4))
rmses

```

\newpage

## (II g) KNN models

Since all variables appeared to improve the LM model above, we also include these in a KNN model, which looks multidimensionally at the nearest neighbors of an observation and predicts from there.

A first try with KNN including elevation yields a less-than-optimal result (see RMSE table below). Another model which excludes elevation worked well. I could not immediately detect the reason for this, but my conclusion is that not every variable improves every type of model in the same way. Generally, KNN performed well after improving on the number of neighbors with a sapply() function.

```{r, include = F}
#### KNN [caret] with N, pH, elevation, landuse ####
# as a rule of thumb, sqrt of observations:
sqrt(nrow(trainset)) # = 125

# after some trials, the following range of ks were found useful
ks <- seq(1,15,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn <- knnreg(OC ~ ., data = trainset, k = i)
  pred <-predict(fit_knn, base)
  RMSE(testset$OC, pred)
})
data.frame(result)[1,]

plot(ks, result)

best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 3
# it appears a better k is lower but I don't want to depend on too few points as this
# may lead to overtraining

best_k <- 10

## using best k

fit_knn <- knnreg(OC ~ ., data = trainset, k = best_k)
pred6 <-predict(fit_knn, base)

qplot(testset$OC, pred6)
rmse6 <- RMSE(testset$OC, pred6)
rmse6

rmses <- rbind(rmses, 
               data.frame(model = "knn with all variables", rmse = rmse6))
rmses
# this is clearly not good
# elevation showed to be the problem
```

```{r echo = T, fig.height=3, fig.width=3}

#### KNN [caret] all variables / excl. elevation ####
sqrt(nrow(trainset)) 

ks <- seq(10,30,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = i)
  pred <-predict(fit_knn2, base)
  RMSE(testset$OC, pred)
})

plot(ks, result)
```

The graph above shows different k's (numbers of neighbors considered) and how the result (the RMSE) behaves with this. A k of 17 was used because it yields the best result.

```{r, echo = T}
best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 17

## using best k

fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = best_k)
pred7 <-predict(fit_knn2, base)

qplot(testset$OC, pred7)
rmse7 <- RMSE(testset$OC, pred7)
rmse7

rmses <- rbind(rmses, 
               data.frame(model = "knn excluding elev.", rmse = rmse7))
rmses

```

\newpage

## (II h) Random forest model

A random forest model consists of many models (trees) that are combined (to a forest) so the number of trees plays a role in the performance of the model. The more trees the more exact the model usually is, but with more trees it also takes more time to calculate. Here, I started with 10 trees, then tried 100, then 250 and then 500. The number of trees (ntree = x) of 250 appeared to yield a good compromise between computing time and performance. The best model includes all selected variables, unlike KNN, which performed best without elevation.

```{r echo = T, fig.height=3, fig.width=3}
set.seed(1)

fit_rf <- randomForest(
  OC ~ ., data = trainset, 
  ntree = 250
)

# going from 10 to 100 trees in the only-N model improved the RMSE by ~0.4
# and another 0.2 for 250 trees, hardly then anymore for 500 trees

pred8 <- predict(fit_rf, base)
head(pred8)

qplot(testset$OC, pred8)
rmse8 <- RMSE(testset$OC, pred8)
rmse8

rmses <- rbind(rmses, 
               data.frame(model = "rf with all variables", rmse = rmse8))
rmses

## in this case all variables improve the model


```

## (II i) Ensemble model (KNN and random forest)

The two models with the best performance (lowest RMSE), were combined into an ensemble, where the mean is taken from the prediction of both models. This worked well as it further improved the RMSE:

```{r echo=T, fig.height=3, fig.width=3}
pred_ensemble <- (pred7 + pred8)/2 # knn & random forest 

qplot(testset$OC, pred_ensemble)
rmse_ens1 <- RMSE(testset$OC, pred_ensemble)
rmse_ens1

rmses <- rbind(rmses, 
               data.frame(model = "ensemble of knn & rf", rmse = rmse_ens1))
```

\newpage

# (III) RESULTS

## (III a) RMSEs of tested models

As a result, we see an order of performance from best to least performing model that is Ensemble (KNN & random forest) > KNN > random forest > linear model.

Here's the overview table:

```{r, echo = T}
rmses
```

## (III b) Evaluation of best model on validation set

To finally evaluate performance of our model on an unseen dataset, we now predict organic carbon in the validation set with the best two models: KNN without elevation and random forest with all variables. It is done as an ensemble.

```{r echo = T, fig.height=3, fig.width=5}
# incorporate the mean prediction of knn and rf since they are the best ones

fin_predict_a <- predict(fit_knn2, validation)
fin_predict_b <- predict(fit_rf, validation)

fin_predict <- (fin_predict_a + fin_predict_b) / 2

qplot(validation$OC, fin_predict, col = validation$landuse)

fin_rmse <- RMSE(validation$OC, fin_predict)

rmses <- rbind(rmses, 
               data.frame(model = "FINAL RMSE OVERALL", rmse = fin_rmse))
```

#### This yields the final RMSE, our typical prediction error, which is:

```{r, echo = T}
fin_rmse
```

# (III b) Model performance in the most common range of organic carbon

In the observation-prediction-graph above, we see that there are ranges of organic where the model performes better than in other ranges. We can divide the validation set into 3 ranges that appear to differ and calculate model performance for each. This can be interesting in practical regards as the user may be interested in certain soils that lay within a specific range of organic carbon.


# (IV) CONCLUSION 


## (IV a) Lessons learned

## (IV b) Limitations

## (IV c) Final thoughts


\newpage