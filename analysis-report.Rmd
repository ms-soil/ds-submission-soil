---
title: "Predicting Organic Carbon in European Soils"
author: "Marcus Schmidt"
date: "23/02/2021"
output: pdf_document
---

# (I) INTRODUCTION

## (I a) Background, goal & data set

Soils are the basis of all agriculture. Organic carbon and its dynamics play a large role carbon sequestration and therefore help regulate our climate. The goal of this project is to predict soil organic carbon from a large set of European soil data. The data set is from the LUCAS initiative 2015 (https://esdac.jrc.ec.europa.eu/projects/lucas) and includes over 20,000 sampling points from the upper soil on land of various land-use types.

## (I b) Data set download

The data set was requested online from the LUCAS initiative and the author got permission to use it for this project. Out of the data set, I created a *.Rdata file to be downloaded here:
```{r, echo = T, results = "hide"}
load(url("https://github.com/ms-soil/ds-submission-soil/raw/main/eusoil.Rdata"))
```

## (I c) Data set structure

Let's take a peak at the datset, showing the variables included, the dimension of the data set and a first look at the numbers and their variable type:

```{r include=FALSE}
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")

library(caret)
library(tidyverse)
library(randomForest)
```

```{r, echo = T}
names(eusoil) # variable names
dim(eusoil) # data set dimensions
```

\newpage

```{r, echo = T}
as_tibble(eusoil) # overview of data structure
```

# (II) METHODS & ANALYSIS

## (II a) Data preparation

Analysis was done with R version 4.0.3. \newline
Important variables for this data set to keep an eye on are: \newline
- OC = organic carbon in mg C per gram soil \newline
- N = total nitrogen in mg N per gram soil & \newline
- Elevation in m. \newline
Land use as a variable in the data set is not so straightforward. It is decoded in the LC1-variable, so we detect the capital letter in the variable character string and assign the respective land use from the data-set documentation, which can be found at https://github.com/ms-soil/ds-submission-soil (data-info.pdf). After this we remove the LC1-variable. We further check whether there are any observations where organic carbon (OC) is missing and take a look at how many observations we have for each land use. For further analysis, land use will be turned into a factor.

```{r, echo = T}
eusoil <- eusoil %>% mutate(landuse = case_when(str_detect(LC1, "A") ~ "artif_land",
                                                str_detect(LC1, "B") ~ "cropland",
                                                str_detect(LC1, "C") ~ "woodland",
                                                str_detect(LC1, "D") ~ "shrubland",
                                                str_detect(LC1, "E") ~ "grassland",
                                                str_detect(LC1, "F") ~ "bare_land",
                                                str_detect(LC1, "G") ~ "water",
                                                str_detect(LC1, "H") ~ "wetland")) %>% 
  select(-LC1)

table(is.na(eusoil$OC)) # organic carbon data is available in all observations
```

Let's take a look at the distribution of samples on different land uses.

```{r, echo = T}
# distribution of landuses
table(eusoil$landuse)

# factorize landuse
eusoil <- eusoil %>% mutate(landuse = as.factor(landuse))
```


## (II b) Variable selection

We want to use a complete data set for our predictions, so we exclude variables that are often missing in the data set. We can see it with the following code. It yields that we are missing the texture variables (sand, silt, clay) in over 80% of cases.
```{r, echo = T, results = "hide"}
# print availability for every variable
res <- for (i in 1:length(names(eusoil))) { 
  print(table(!is.na(eusoil[,i])))
  print(names(eusoil[i]))
}
# texture (the silt, sand and clay variables) are only available in a fraction of cases
# see which fraction of the dataset misses texture
condition <- !is.na(eusoil$Clay)
table(condition)[[1]]/(table(condition)[[1]] + table(condition)[[2]])
# texture is missing in 80.5% of cases
```
Here is a bit of the result:
```{r, echo = T}
# FALSE  TRUE 
# 17599  4260 
# [1] "Clay"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Sand"
# 
# FALSE  TRUE 
# 17599  4260 
# [1] "Silt"
```

The texture variables clay, sand and silt are not often measured in this data set so we exclude them. We also exclude other variables that are not commonly measured by soil scientists or not clearly defined for this data set:

```{r, echo = T}
eusoil <- eusoil %>% select(-Clay, -Sand, -Silt, -EC, -pH.CaCl2., -Soil_Stones, -LC1_Desc) %>% 
  select(OC, N, P, K, CaCO3, pH.H2O., Elevation, landuse)
```

Which variables are left and how do they relate? We want to draw some conclusions of what to include by looking at a correlation plot.

```{r, echo = T, message = F, results = "hide"}
#### variable selection III: influential variables ####
names(eusoil)
## a) numerical variables that correlate with organic carbon but not with each other
relationships <- plot(eusoil[1:2000,]) # plotting the first 2000 observations
relationships
```

P (phosphorus) and K (potassium) show no clear relationship to organic carbon so we will exclude them soon in the code below. CaCO3 and pH are co-dependend so it will be enough to include one of them to represent both. At this point, we are keeping N (Nitrogen), pH and elevation in our analysis

There is also a categorical variable, land use, so we check whether it influences organic carbon (OC):

```{r, echo = T}
qplot(landuse, OC, data = eusoil, fill = landuse) + geom_boxplot() + 
  ylab("organic carbon mg C / g soil")
```

Organic carbon differs with land use so we keep it as a factor. Now that the decision on the variables has been made, we give out some easier-to-type variable names and select the chosen variables from the data set.

```{r, echo = T}
eusoil <- eusoil %>% mutate(pH = pH.H2O., elev = Elevation) %>% 
  select(OC, N, P, pH, elev, landuse)
```

## (II c) Data set division into validation, train and test set

First, from our data set, a validation set of 10% is taken which will only be used after deciding on a model. The validation set is important so that our model will not be fit to a specific data set. We keep a large proportion of the data for model training. The training set will be 80% and the test set 20% of the remaining data. We should choose a split that allows lots of training but still a representative test set. For reproducible results, we use the set.seed() function.

```{r, echo = T, warning = F}
set.seed(1, sample.kind = "Rounding")
valindex <- createDataPartition(eusoil$OC, p = 0.1, list = F)
temp <- eusoil[-valindex,]
validation <- eusoil[valindex,]

set.seed(1, sample.kind = "Rounding")
testindex <- createDataPartition(temp$OC, p = 0.2, list = F)
trainset <- temp[-testindex,]
testset <- temp[testindex,]

# see how many observations are in each
nrow(trainset); nrow(testset); nrow(validation)
```
The split retains 2000 obervations for the final validation set which should be plenty.

## (II d) RMSE function

A rooted mean square error (RMSE) function is used to evaluate the performance of the models that will be set up. It can be viewed as the typical error we make when predicting from a given model.

```{r, echo = T}
RMSE <- function(observed_values, predicted_values){
  sqrt(mean((observed_values - predicted_values)^2))
}
```


## (II e) Reference model
A reference model can be useful to evaluate how different models perform compared to simply guessing the outcome. In guessing, we would take the average of the observed carbon values.

```{r echo = T, fig.height=3, fig.width=3}
#### guessing model ####
mu <- mean(trainset$OC)

rmse_mu <- RMSE(testset$OC, mu)
rmse_mu

# create an RMSE table to always add the RMSEs
rmses <- data.frame(model = "mean", rmse = rmse_mu)
rmses
```

We see that if we guess, we are typically ~78 mg C / g soil off the observed value. We will improve this prediction, first with linear models but also with KNN and random forest algorithms.


## (II f) Linear models

In the following, different linear models (LM) are set up, first with N (nitrogen) as predictor, then adding pH, elevation and landuse. The full code is present in the R script and not shown here as to not overload the report, However, we show the LM which includes all variables and the table of how RMSE impoved step by step when adding more variables.

```{r, include = F}
#### LM with N (Nitrogen) ####

m1 <- lm(OC ~ N, data = trainset) 

# the base will be the testset as a dataframe to predict on
base <- data.frame(testset)
pred1 <- predict(m1, base)
head(pred1)

# view predictions
qplot(testset$OC, pred1, col = testset$landuse)

# get RMSE
rmse1 <- RMSE(testset$OC, pred1)
rmse1

# add RMSE to RMSE table
rmses <- rbind(rmses, 
               data.frame(model = "lm with N", rmse = rmse1))
rmses

#### LM with N, pH #### 
# for comments on the steps, see above

m2 <- lm(OC ~ N + pH, data = trainset)
summary(m2)

base <- data.frame(testset)

pred2 <- predict(m2, base)
head(pred2)

qplot(testset$OC, pred2)
rmse2 <- RMSE(testset$OC, pred2)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH", rmse = rmse2))
rmses


#### LM with N, pH, elevation ####

m3 <- lm(OC~N + pH + elev, data = trainset)
summary(m3)

base <- data.frame(testset)

pred3 <- predict(m3, base)
head(pred3)

qplot(testset$OC, pred3)
rmse3 <- RMSE(testset$OC, pred3)

rmses <- rbind(rmses, 
               data.frame(model = "lm with N + pH + elev.", rmse = rmse3))
rmses
```

```{r echo = T, fig.height=3, fig.width=3}
#### lm with N, pH, elevation, landuse ####

m_all <- lm(OC ~ ., data = trainset)

base <- data.frame(testset)

pred4 <- predict(m_all, base)
head(pred4)

qplot(testset$OC, pred4)
rmse4 <- RMSE(testset$OC, pred4)
rmse4

rmses <- rbind(rmses, 
               data.frame(model = "lm with all variables", rmse = rmse4))
rmses

```

\newpage

## (II g) KNN models

Since all variables appeared to improve the LM model above, we also include these in a KNN model, which looks multidimensionally at the nearest neighbors of an observation and predicts from there.

A first try with KNN including elevation yields a less-than-optimal result (see RMSE table below). Another model which excludes elevation worked well. I could not immediately detect the reason for this, but my conclusion is that not every variable improves every type of model in the same way. Generally, KNN performed well after improving on the number of neighbors with a sapply() function.

```{r, include = F}
#### KNN [caret] with N, pH, elevation, landuse ####
# as a rule of thumb, sqrt of observations:
sqrt(nrow(trainset)) # = 125

# after some trials, the following range of ks were found useful
ks <- seq(1,15,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn <- knnreg(OC ~ ., data = trainset, k = i)
  pred <-predict(fit_knn, base)
  RMSE(testset$OC, pred)
})
data.frame(result)[1,]

plot(ks, result)

best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 3
# it appears a better k is lower but I don't want to depend on too few points as this
# may lead to overtraining

best_k <- 10

## using best k

fit_knn <- knnreg(OC ~ ., data = trainset, k = best_k)
pred6 <-predict(fit_knn, base)

qplot(testset$OC, pred6)
rmse6 <- RMSE(testset$OC, pred6)
rmse6

rmses <- rbind(rmses, 
               data.frame(model = "knn with all variables", rmse = rmse6))
rmses[5:7,]
# this is clearly not good
# elevation showed to be the problem
```

```{r echo = T, fig.height=3, fig.width=3}

#### KNN [caret] all variables / excl. elevation ####
sqrt(nrow(trainset)) 

ks <- seq(10,30,1)
ks

# checking which is the optimal k
result <- lapply(ks, function(i){
  fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = i)
  pred <-predict(fit_knn2, base)
  RMSE(testset$OC, pred)
})

plot(ks, result)
```

The graph above shows different k's (numbers of neighbors considered) and how the result (the RMSE) behaves with this. A k of 17 was used because it yields the best result.

```{r, echo = T, fig.height=3, fig.width=3}
best_k <- ks[which.min(data.frame(result)[1,])] #31
best_k # this is 17

## using best k

fit_knn2 <- knnreg(OC ~ N + pH + landuse, data = trainset, k = best_k)
pred7 <-predict(fit_knn2, base)

qplot(testset$OC, pred7)
rmse7 <- RMSE(testset$OC, pred7)
rmse7

rmses <- rbind(rmses, 
               data.frame(model = "knn excluding elev.", rmse = rmse7))
rmses[5:7,]

```

\newpage

## (II h) Random forest model

A random forest model consists of many models (trees) that are combined (to a forest) so the number of trees plays a role in the performance of the model. The more trees the more exact the model usually is, but with more trees it also takes more time to calculate. Here, I started with 10 trees, then tried 100, then 250 and then 500. The number of trees (ntree = x) of 250 appeared to yield a good compromise between computing time and performance. The best model includes all selected variables, unlike KNN, which performed best without elevation. However, the random forest model is not quite as good as the KNN was.

```{r echo = T, fig.height=3, fig.width=3}
set.seed(1)

fit_rf <- randomForest(
  OC ~ ., data = trainset, 
  ntree = 250
)

# going from 10 to 100 trees in the only-N model improved the RMSE by ~0.4
# and another 0.2 for 250 trees, hardly then anymore for 500 trees

pred8 <- predict(fit_rf, base)
head(pred8)

qplot(testset$OC, pred8)
rmse8 <- RMSE(testset$OC, pred8)
rmse8

rmses <- rbind(rmses, 
               data.frame(model = "rf with all variables", rmse = rmse8))
rmses[7:8,]

## in this case all variables improve the model


```

## (II i) Ensemble model (KNN and random forest)

The two models with the best performance (lowest RMSE), were combined into an ensemble, where the mean is taken from the prediction of both models. This worked well as it further improved the RMSE:

```{r echo=T, fig.height=3, fig.width=3}
pred_ensemble <- (pred7 + pred8)/2 # knn & random forest 

qplot(testset$OC, pred_ensemble)
rmse_ens1 <- RMSE(testset$OC, pred_ensemble)
rmse_ens1

rmses <- rbind(rmses, 
               data.frame(model = "ensemble of knn & rf", rmse = rmse_ens1))

rmses[7:9,]
```

# (III) RESULTS

## (III a) RMSEs of tested models

As a result, we see an order of performance from best to least performing model that is Ensemble (KNN & random forest) > KNN > random forest > linear model.

Here's the overview table:

```{r, echo = T}
rmses
```

## (III b) Evaluation of best model on validation set

To finally evaluate performance of our model on an unseen dataset, we now predict organic carbon in the validation set with the best two models: KNN without elevation and random forest with all variables. It is done as an ensemble.

```{r echo = T, fig.height=3, fig.width=5}
# incorporate the mean prediction of knn and rf since they are the best ones

fin_predict_a <- predict(fit_knn2, validation)
fin_predict_b <- predict(fit_rf, validation)

fin_predict <- (fin_predict_a + fin_predict_b) / 2

qplot(validation$OC, fin_predict, col = validation$landuse) +
  geom_vline(xintercept = 150, lty = 2) + geom_vline(xintercept = 400, lty = 2)

fin_rmse <- RMSE(validation$OC, fin_predict)

rmses <- rbind(rmses, 
               data.frame(model = "FINAL RMSE OVERALL", rmse = fin_rmse))
```

The dashed lines represent areas where the model performs differently. We will come back to this soon, but let's first calculate the final RMSE overall.

#### The final RMSE, our typical prediction error, is:

```{r, echo = T}
fin_rmse
```

# (III b) Model performance in the most common range of organic carbon

In the observation-prediction-graph above, we see that there are ranges of organic carbon in which the model performs better than in other ranges. We can divide the validation set into three ranges that appear to differ and calculate model performance for each. This can be interesting in practical regards as the user may be interested in certain soils that lay within a specific range of organic carbon. I show here how it wsa done for the 0-150 mg C / g soil range and then performance in each range as a table:

```{r, echo = T}
# I put the observations, predictions and land uses in a data frame
df <- data.frame(landuse = validation$landuse,
                 observation = validation$OC, prediction = fin_predict)

# in the following, I divide this set and observe the model performance on
# data ranges

# range 0-150
obs  <- df$observation[df$observation < 150]
pred <- df$prediction[df$observation < 150]

fin_rmse_0_150 <- RMSE(obs, pred)
fin_rmse_0_150
```

It is of interest which fraction of observations within this range that yields the best result. It turns out to be 95% of all observations.
```{r, echo = T}
length(eusoil$OC[eusoil$OC < 150]) / 
  length(eusoil$OC[eusoil$OC])
# it is 95% of observations
```

```{r, echo = F, results = "hide"}
# range 150-400
obs  <- df$observation[df$observation >= 150 & df$observation <= 400]
pred <- df$prediction[df$observation >= 150 & df$observation <= 400]

fin_rmse_150_400 <- RMSE(obs, pred)
fin_rmse_150_400

# range 400+
obs  <- df$observation[df$observation > 400]
pred <- df$prediction[df$observation > 400]

fin_rmse_400_plus <- RMSE(obs, pred)
fin_rmse_400_plus


rmses <- rbind( 
               data.frame(model = "range <150 mg/g org. C", rmse = fin_rmse_0_150),
               data.frame(model = "range 150-400 mg/g org. C", rmse = fin_rmse_150_400),
               data.frame(model = "range 400+ mg/g org. C", rmse = fin_rmse_400_plus))
```

For the selected ranges, the model performs as follows:

```{r, echo = T}
rmses
```

We see that performance is best in a range lower than 150 mg C / g soil with a typical error of just 10.2 mg C / g soil. How does this error spread over the different land uses? 

```{r, echo = T}
df %>% filter(observation < 150) %>% mutate(error = observation - prediction) %>%
  ggplot(aes(landuse, error, col = landuse)) +
  geom_hline(yintercept = 0)+ geom_jitter(alpha = 0.5) +
  theme_bw() + ylab("Absolute error in predicted organic carbon in mg C/g soil") +
  xlab("Land use") + theme(legend.position = "none") +
  annotate(geom = "text", label = "underestimating", x = 1.5, y = 75, col = "darkgreen") +
  annotate(geom = "text", label = "overestimating", x = 1.5, y = -75, col = "darkgreen") 
```

# (IV) CONCLUSION 

## (IV a) Project summary

Using the soil parameters total P (phosphorus), pH, elevation and landuse, we were able to predict soil organic carbon with a typical error of 10.2 mg C/g soil in the range of organic carbon below 150 mg C/g soil. The predictions were relatively consistent for all land uses. It paid off to use different modelling approaches and do some fine-tuning of these models as well. In the end, a combination of KNN and random forest performed best.

## (IV b) Limitations
There are of course several more approaches that can be done and an ensemble of many more models is very likely to further improve the result. Also more environmental variables like precipitation, temperature and consistent availability of soil texture would likely improve the model.

## (IV c) Future applications
Predicting soil organic carbon is just one example of what can be done. Within large datasets, often some soil variables are missing and those could, in a certain range of accuracy, be predicted from those variables that are present to fill in the gaps. Another application is the large-scale prediction of a variable that is complicated to measure and only available for a fraction of the area of interest. Machine learning algorithms can be used to predict those variables. When we use such algorithms in a transparent way and are open about their strength, limiations and range of error, they can be very useful in giving insights that we otherwise would not obtain - which is what data science is all about.

This report together with the \*.R analysis file, the \*.Rmd file and the *Rdata file used can be found at my github repository at: https://github.com/ms-soil/ds-submission-soil


